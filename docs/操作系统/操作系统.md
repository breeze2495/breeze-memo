## 



## 一 操作系统基础





## 二 进程与线程

### 2.1 进程与线程

- **进程：** 
  - 一个正在执行的程序。
  - 资源调度的基本单位
- **线程：**
  - 一个进程由多个线程组成
  - CPU调度与执行的基本单位



### 2.2 进程的几种状态

- **创建状态(new)** ：进程正在被创建，尚未到就绪状态。
- **就绪状态(ready)** ：进程已处于准备运行状态，即进程获得了除了处理器之外的一切所需资源，一旦得到处理器资源(处理器分配的时间片)即可运行。
- **运行状态(running)** ：进程正在处理器上上运行(单核 CPU 下任意时刻只有一个进程处于运行状态)。
- **阻塞状态(waiting)** ：又称为等待状态，进程正在等待某一事件而暂停运行如等待某资源为可用或等待 IO 操作完成。即使处理器空闲，该进程也不能运行。
- **结束状态(terminated)** ：进程正在从系统中消失。可能是进程正常结束或其他原因中断退出运行。



### 2.3 进程间的通信方式

`(IPC Internal - Process - C)`

#### 2.3.1 管道 / 匿名管道 (pipe)

- 管道是**半双工的**，实质是一个**内核缓冲区**，一端将数据写入，另一端读取
- 只能用于**父子进程或者兄弟进程**之间(具有**亲缘关系**的进程)
- 单独构成一个文件系统(不属于某种文件系统)，只存在于内存中
- 当缓冲区读空或者写满时，有一定的规则会控制相应的读/写进程进入等待队列

- **局限性:**
  - 单项数据流
  - 只能具有亲缘关系的进程间
  - 缓冲区有限（存在于内存中，管道创建时，为缓冲区分配一个页面大小
  - 管道传送的是无格式字节流，这就要求管道读写两端事先约定好数据格式，比如多少字节算一个消息等等



#### 2.3.2 有名管道 (FIFO)

- 匿名管道由于没有名字只能用于亲缘关系的进程间通信。为了克服这一缺点，提出了有名管道
- 提供了一个**路径名**与之关联，因此不存在亲缘关系的进程，只要访问该路径，就能彼此通过有名管道通信
- **先进先出**，不支持诸如lseek()等文件定位操作
- 名字存在于文件系统中，内容存放在内存中



#### 2.3.3 信号 (signal)

- 信号时linux系统中用于进程间互相通信或者操作的一种机制，信号可以在任何时候发给某一进程而无需知道该进程的状态
- 如果该进程处于阻塞状态，则该信号就有内核将其保存起来，直到该进程恢复执行并传递给它为止
- 如果一个信号被进程设置为阻塞，则该信号的传递被延迟，直到其阻塞被取消才被传递给该进程



#### 2.3.4 消息队列(message queue)

- 消息队列是存放在内核中的消息链表，具有特定格式，每个消息队列由消息队列标识符表示
- 与管道不同的是消息队列存放在内核中，只有在内核重启或者显示删除时，该消息队列才会被真正删除
- 另外与管道不同的是，消息队列在某个进程往一个队列写入之前，并不需要另外某个进程在该队列上等待消息的到达

- **特点：**
  - 允许一个或多个进程向它写入/读取消息
  - 可以实现消息的随机查询，不一定以先进先出的次序读取，也可以按消息的类型读取，比FIFO更有优势
  - 克服了信号承载信息量少，管道只能承载无格式字节流以及管道缓冲区大小受限 等缺陷
  - 目前主要有两种类型的消息队列：
    - Posix消息队列
    - System V消息队列(目前大量使用)： 随内核持续，只有在内核重启或者人工删除时，该消息队列才会被删除



#### 2.3.5 共享内存(share memory)

- 多个进程可以直接读写同一块内存空间，是最快的可用IPC方式
- 为了多个进程间交换信息，内核专门留出一块内存区，可以由需要访问的进程将其映射到自己的私有地址空间。进程就可以直接读写这一块内存而不需要进行数据的拷贝，从而大大地提高了效率
- 由于多个进程共享一段内存，因此需要依靠某种同步机制(如信号量)来达到进程间的同步及互斥



#### 2.3.6 信号量(semaphore)

- 信号量是一个计数器，用于多进程对共享数据的访问，信号量的意图在于进程间同步

- 为了获得共享资源，进程需要执行下列操作：
   （1）**创建一个信号量**：这要求调用者指定初始值，对于二值信号量来说，它通常是1，也可是0。
   （2）**等待一个信号量**：该操作会测试这个信号量的值，如果小于0，就阻塞。也称为P操作。
   （3）**挂出一个信号量**：该操作将信号量的值加1，也称为V操作。

- 为了正确地实现信号量，信号量值的测试及减1操作应当是原子操作。为此，信号量通常是在内核中实现的。Linux环境中，有三种类型：**Posix（可移植性操作系统接口）有名信号量（使用Posix IPC名字标识）**、**Posix基于内存的信号量（存放在共享内存区中）**、**System V信号量（在内核中维护）**。这三种信号量都可用于进程间或线程间的同步。

  <img src="https://gitee.com/breeze1002/upic/raw/master/OS/os-1/2021%2010%2028%2010%2038%2003%201635388683%201635388683125%20ZemYsQ%20image-20210820093852923.png" alt="image-20210820093852923" style="zoom:35%" />

  

  > **信号量与普通整型变量的区别：**
  >  （1）信号量是非负整型变量，除了初始化之外，它只能通过两个标准原子操作：wait(semap) , signal(semap) ; 来进行访问；
  >  （2）操作也被成为PV原语（P来源于荷兰语proberen"测试"，V来源于荷兰语verhogen"增加"，P表示通过的意思，V表示释放的意思），而普通整型变量则可以在任何语句块中被访问；

  

> **信号量与互斥量之间的区别：**
>  （1）互斥量用于线程的互斥，信号量用于线程的同步。这是互斥量和信号量的根本区别，也就是互斥和同步之间的区别。
>  **互斥：**是指某一资源同时只允许一个访问者对其进行访问，具有唯一性和排它性。但互斥无法限制访问者对资源的访问顺序，即访问是无序的。
>  **同步：**是指在互斥的基础上（大多数情况），通过其它机制实现访问者对资源的有序访问。
>  在大多数情况下，同步已经实现了互斥，特别是所有写入资源的情况必定是互斥的。少数情况是指可以允许多个访问者同时访问资源
>  （2）互斥量值只能为0/1，信号量值可以为非负整数。
>  也就是说，一个互斥量只能用于一个资源的互斥访问，它不能实现多个资源的多线程互斥问题。信号量可以实现多个同类资源的多线程互斥和同步。当信号量为单值信号量是，也可以完成一个资源的互斥访问。
>  （3）互斥量的加锁和解锁必须由同一线程分别对应使用，信号量可以由一个线程释放，另一个线程得到。



#### 2.3.7 套接字(socket)

- 套接字是一种通信机制，凭借这种机制，客户端/服务器可以通过网络实现进程间的通信
- 套接字时支持TCP/IP通信的基本操作单元，可以看做不同主机之间进程进行双向通信的端点





### 2.4  线程间的同步方式

线程同步是两个或多个共享关键资源的线程的并发执行。应该同步线程以避免关键资源使用冲突。

- **互斥量：** 采用互斥对象机制。只有拥有互斥对象的线程才有访问公共资源的权限。因为互斥对象只有一个，因此可以保证公共资源不会被多个线程同时访问。  (例如 Synchronized关键字 / Lock锁)
- **信号量：** 允许同一时刻多个线程访问同一资源，但需要控制同一时间访问该资源的最大线程数
- **事件：** wait/notify：通过通知操作的方式保持线程同步，还可以实现多线程优先级的比较操作



### 2.5 进程的调度算法

`为了确定进程的执行顺序以实现最大的CPU利用率`

- **先到先服务(FCFS):**从就绪队列中选择一个最先进入该队列的进程为其分配资源，并立即执行
- **短作业优先(SJF):** 从就绪队列中选择一个估计运行时间最短的进程为之分配资源，并立即执行
- **时间片轮转:** 每个进程被分配一个时间片来运行
- **多级反馈队列:** 短作业优先算法有一定的局限性(只照顾短进程而忽略了长进程)。 而多级反馈队列能使高优先级的作业得到响应又能使短作业迅速完成。因而时目前公认的一种较好的进程调度算法 （UNIX操作系统采用）
- **优先级调度:** 为每个进程分配优先级，首先执行优先级最高的进程，以此类推。相同的优先级采用FCFS  



## 三 操作系统内存管理基础

- **内存空间的分配与回收**
- **内存空间的扩充**
- **地址转换**
- **存储保护**





### 3.1 内存管理机制

- **连续分配管理方式:** 分配连续的内存空间
  - **块式管理**
    - 远古时代的计算机操系统的内存管理方式。将内存分为几个固定大小的块，每个块中只包含一个进程。如果程序运行需要内存的话，操作系统就分配给它一块，如果程序运行只需要很小的空间的话，分配的这块内存很大一部分几乎被浪费了。这些在每个块中未被利用的空间，我们称之为碎片
- **非连续分配管理方式:** 离散的内存空间
  - **页式管理**
    - 把主存分为大小相等且固定的一页一页的形式，页较小，相对相比于块式管理的划分力度更大，提高了内存利用率，减少了碎片。页式管理通过页表对应逻辑地址和物理地址。
  - **段式管理**
    - 页式管理虽然提高了内存利用率，但是页式管理其中的页实际并无任何实际意义。 段式管理把主存分为一段段的，每一段的空间又要比一页的空间小很多 。但是，最重要的是段是有实际意义的，每个段定义了一组逻辑信息，例如,有主程序段 MAIN、子程序段 X、数据段 D 及栈段 S 等。 段式管理通过段表对应逻辑地址和物理地址。
  - **段页式管理机制**
    - 段页式管理机制结合了段式管理和页式管理的优点。简单来说段页式管理机制就是把主存先分成若干段，每个段又分成若干页，也就是说 **段页式管理机制** 中段与段之间以及段的内部的都是离散的。
- **分页机制与分段机制**
  - **共同点**
    - 都是为了提高内存利用率而生，会产生较少的内存碎片
    - 都是离散的分配管理方式。但是每个段/页内时连续的
  - **区别**
    - 页的大小固定，由操作系统决定 ； 段的大小不固定，取决于当前运行的程序
    - 分页仅仅是满足操作系统 内存管理的需求；而段是逻辑信息的单位，提现为可以区分为代码段，数据段等等



### 3.2 页表管理/快表/多级页表

- **页表管理机制**

  - 实现从进程的逻辑地址转换为内存的物理地址

    <img src="https://gitee.com/breeze1002/upic/raw/master/OS/os-1/2021%2010%2028%2010%2038%2006%201635388686%201635388686367%20BTlbmu%20image-20210822090212270.png" alt="image-20210822090212270" style="zoom:50%" />



- **快表**
  - **页表 存在的问题**
    - 页表存在与内存中，因此CPU存取一个数据，需要访问主存两次
      - 第一次：访问内存中的页表，找到物理块号，将此块号与业内地址拼接形成物理地址
      - 第二次：真正访问该物理地址，存取其中内容
    - 这样就把程序执行速度降低一倍，为了提高存取速度，在地址变换机构中增设一组寄存器，用于存放页表
  - **存放在高速缓冲寄存器(联想存贮器TLB)中的页表成为快表，** 内存中的页表成为慢表

- **多级页表**
  - 现在的计算机系统，支持非常大的逻辑地址空间(232 - 264)。因此页表就变得非常大，需要占用很大的内存空间
  - **利用局部性原理**    牺牲时间换取空间
    - **二级页表可以不存在：**  一级页表会覆盖整个虚拟地址空间，但是如果某个一级页表的页表项没有被用到，也就不需要创建这个页表对应的二级页表了。假设只有20%的一级页表项被使用到，那就会节省非常大的内存空间；
    - **二级页表可以不存在主存中：** 将二级页表存储在磁盘,需要时再去调用











## 四 虚拟内存

### 4.1 概念

虚拟内存是一种计算机系统的**内存管理技术**，它可以:

- **让程序可以拥有超过系统物理内存大小的可用内存空间**
- **为每个进程提供一片连续完整的内存空间，让每个进程产生一种自己在独享主存的错觉。**

> 实际上，它通常被分隔成多个物理内存碎片，还有部分暂时存储在硬盘上，在需要时进行数据交换。
>
> 与没有使用虚拟内存技术的系统相比，使用这种技术的系统使得大型程序的编写变得更容易，对真正的物理内存（例如 RAM）的使用也更有效率。目前，大多数操作系统都使用了虚拟内存，如 Windows 家族的“虚拟内存”；Linux 的“交换空间”等



### 4.2 局部性原理

**局部性原理是虚拟内存技术的基础，正是因为程序运行具有局部性原理，才可以只装入部分程序到内存就开始运行。**

> 以下内容摘自《计算机操作系统教程》 第 4 章存储器管理。

早在 1968 年的时候，就有人指出我们的程序在执行的时候往往呈现局部性规律，也就是说在某个较短的时间段内，程序执行局限于某一小部分，程序访问的存储空间也局限于某个区域。

局部性原理表现在以下两个方面：

1. **时间局部性** ：如果程序中的某条指令一旦执行，不久以后该指令可能再次执行；如果某数据被访问过，不久以后该数据可能再次被访问。产生时间局部性的典型原因，是由于在程序中存在着大量的循环操作。
2. **空间局部性** ：一旦程序访问了某个存储单元，在不久之后，其附近的存储单元也将被访问，即程序在一段时间内所访问的地址，可能集中在一定的范围之内，这是因为指令通常是顺序存放、顺序执行的，数据也一般是以向量、数组、表等形式簇聚存储的。

- **时间局部性**是通过将近来使用的指令和数据保存到高速缓存存储器中，并使用高速缓存的层次结构实现。
- **空间局部性**通常是使用较大的高速缓存，并将预取机制集成到高速缓存控制逻辑中实现。
- 虚拟内存技术实际上就是建立了 “内存一外存”的两级存储器的结构，利用局部性原理实现髙速缓存。



### 4.3 虚拟内存的技术实现

**虚拟内存的实现需要建立在离散分配的内存管理方式的基础上。** 虚拟内存的实现有以下三种方式：

1. **请求分页存储管理** ：建立在分页管理之上，为了支持虚拟存储器功能而增加了请求调页功能和页面置换功能。请求分页是目前最常用的一种实现虚拟存储器的方法。请求分页存储管理系统中，在作业开始运行之前，仅装入当前要执行的部分段即可运行。假如在作业运行的过程中发现要访问的页面不在内存，则由处理器通知操作系统按照对应的页面置换算法将相应的页面调入到主存，同时操作系统也可以将暂时不用的页面置换到外存中。
2. **请求分段存储管理** ：建立在分段存储管理之上，增加了请求调段功能、分段置换功能。请求分段储存管理方式就如同请求分页储存管理方式一样，在作业开始运行之前，仅装入当前要执行的部分段即可运行；在执行过程中，可使用请求调入中断动态装入要访问但又不在内存的程序段；当内存空间已满，而又需要装入新的段时，根据置换功能适当调出某个段，以便腾出空间而装入新的段。
3. **请求段页式存储管理**

**这里多说一下？很多人容易搞混请求分页与分页存储管理，两者有何不同呢？**

请求分页存储管理建立在分页管理之上。他们的根本区别是是否将程序全部所需的全部地址空间都装入主存，这也是请求分页存储管理可以提供虚拟内存的原因，我们在上面已经分析过了。

它们之间的根本区别在于是否将一作业的全部地址空间同时装入主存。请求分页存储管理不要求将作业全部地址空间同时装入主存。基于这一点，请求分页存储管理可以提供虚存，而分页存储管理却不能提供虚存。

不管是上面那种实现方式，我们一般都需要：

1. 一定容量的内存和外存：在载入程序的时候，只需要将程序的一部分装入内存，而将其他部分留在外存，然后程序就可以执行了；
2. **缺页中断**：如果**需执行的指令或访问的数据尚未在内存**（称为缺页或缺段），则由处理器通知操作系统将相应的页面或段**调入到内存**，然后继续执行程序；
3. **虚拟地址空间** ：逻辑地址到物理地址的变换。



### 页面置换算法

地址映射过程中，若在页面中发现所要访问的页面不在内存中，则发生缺页中断 。

当发生缺页中断时，如果当前内存中并没有空闲的页面，操作系统就必须在内存选择一个页面将其移出内存，以便为即将调入的页面让出空间。用来**选择淘汰哪一页的规则叫做页面置换算法**，我们可以把页面置换算法看成是淘汰页面的规则。

- **OPT 页面置换算法（最佳页面置换算法）** ：最佳(Optimal, OPT)置换算法所选择的被淘汰页面将是以后永不使用的，或者是在最长时间内不再被访问的页面,这样可以保证获得最低的缺页率。但由于人们目前无法预知进程在内存下的若千页面中哪个是未来最长时间内不再被访问的，因而该算法无法实现。一般作为衡量其他置换算法的方法。
- **FIFO（First In First Out） 页面置换算法（先进先出页面置换算法）** : 总是淘汰最先进入内存的页面，即选择在内存中驻留时间最久的页面进行淘汰。
- **LRU （Least Recently Used）页面置换算法（最近最久未使用页面置换算法）** ：LRU算法赋予每个页面一个访问字段，用来记录一个页面自上次被访问以来所经历的时间 T，当须淘汰一个页面时，选择现有页面中其 T 值最大的，即最近最久未使用的页面予以淘汰。
- **LFU （Least Frequently Used）页面置换算法（最少使用页面置换算法）** : 该置换算法选择在之前时期使用最少的页面作为淘汰页。



## 五 I/O多路复用

> 参考文档地址：https://mp.weixin.qq.com/s/Qpa0qXxuIM8jrBqDaXmVNA



### 5.1 最基本的Socket模型

Socket连接是基于 TCP/IP协议栈的， 在socket连接的过程中，必然会有TCP连接的过程。



TCP连接过程中，服务器的内核实际上为每个 Socket 维护了两个队列：

- 一个是还没完全建立连接的队列，称为 **TCP 半连接队列**，这个队列都是没有完成三次握手的连接，此时服务端处于 `syn_rcvd` 的状态；
- 一个是一件建立连接的队列，称为 **TCP 全连接队列**，这个队列都是完成了三次握手的连接，此时服务端处于 `established` 状态；



当 TCP 全连接队列不为空后，服务端的 `accept()` 函数，就会从内核中的 TCP 全连接队列里拿出一个已经完成连接的  Socket 返回应用程序，后续数据传输都用这个 Socket。



注意，监听的 Socket 和真正用来传数据的 Socket 是两个：

- 一个叫作**监听 Socket**；
- 一个叫作**已连接 Socket**；



基于 Linux 一切皆文件的理念，在内核中 Socket 也是以「文件」的形式存在的，因此也有对应的文件描述符。





### 5.2 Socket连接受限

前面提到的 TCP Socket 调用流程是最简单、最基本的，它基本只能一对一通信，因为使用的是**同步阻塞**的方式，当服务端在还没处理完一个客户端的网络 I/O 时，或者读写操作发生阻塞时，其他客户端是无法与服务端连接的。



**服务器单机理论：**

TCP 连接是由四元组唯一确认的，这个四元组就是：**本机IP, 本机端口, 对端IP, 对端端口**。

服务器作为服务方，通常会在本地固定监听一个端口，等待客户端的连接。因此服务器的本地 IP 和端口是固定的，于是对于服务端 TCP 连接的四元组只有对端 IP 和端口是会变化的，所以**最大 TCP 连接数 = 客户端 IP 数×客户端端口数**。

对于 IPv4，客户端的 IP 数最多为 2 的 32 次方，客户端的端口数最多为 2 的 16 次方，也就是**服务端单机最大 TCP 连接数约为 2 的 48 次方**。

这个理论值相当“丰满”，但是服务器肯定承载不了那么大的连接数，主要会受两个方面的限制：

- **文件描述符**，Socket 实际上是一个文件，也就会对应一个文件描述符。在 Linux 下，单个进程打开的文件描述符数是有限制的，没有经过修改的值一般都是 **1024**，不过我们可以通过 ulimit 增大文件描述符的数目；
- **系统内存**，每个 TCP 连接在内核中都有对应的数据结构，意味着每个连接都是会占用一定内存的；



> 那如果服务器的内存只有 2 GB，网卡是千兆的，能支持并发 1 万请求吗？
>
> 并发 1 万请求，也就是经典的 C10K 问题 ，C 是 Client 单词首字母缩写，C10K 就是单机同时处理 1 万个请求的问题。
>
> 从硬件资源角度看，对于 2GB 内存千兆网卡的服务器，如果每个请求处理占用不到 200KB 的内存和 100Kbit 的网络带宽就可以满足并发 1 万个请求。
>
> 不过，要想真正实现 C10K 的服务器，要考虑的地方在于服务器的网络 I/O 模型，效率低的模型，会加重系统开销，从而会离 C10K 的目标越来越远。



### 5.3 传统解决方式: 多进程模型

基于最原始的阻塞网络 I/O， 如果服务器要支持多个客户端，其中比较传统的方式，就是使用**多进程模型**，也就是为每个客户端分配一个进程来处理请求。

服务器的主进程负责监听客户的连接，一旦与客户端连接完成，accept() 函数就会返回一个「已连接 Socket」，这时就通过 `fork()` 函数创建一个子进程，实际上就把父进程所有相关的东西都**复制**一份，包括文件描述符、内存地址空间、程序计数器、执行的代码等。

子进程会**复制父进程的文件描述符**，于是就可以直接使用「已连接 Socket 」和客户端通信了，

- 子进程只需要关心「已连接 Socket」；
- 父进程只需要关心「监听 Socket」。

下面这张图描述了从连接请求到连接建立，父进程创建生子进程为客户服务。

![image-20220104150534717](/Users/breeze/Library/Application Support/typora-user-images/image-20220104150534717.png)

另外，当「子进程」退出时，实际上内核里还会保留该进程的一些信息，也是会占用内存的，如果不做好“回收”工作，就会变成**僵尸进程**，随着僵尸进程越多，会慢慢耗尽我们的系统资源。

因此，父进程要“善后”好自己的孩子，怎么善后呢？那么有两种方式可以在子进程退出后回收资源，分别是调用 `wait()` 和 `waitpid()` 函数。



**存在缺陷**

这种用多个进程来应付多个客户端的方式，在应对少量客户端还是可行的，但是当客户端数量高达一万时，肯定扛不住的，因为每产生一个进程，必会占据一定的系统资源，而且进程间上下文切换的“包袱”是很重的，性能会大打折扣。

进程的上下文切换不仅包含了虚拟内存、栈、全局变量等用户空间的资源，还包括了内核堆栈、寄存器等内核空间的资源。



### 5.4 多线程模型

相较于进程切换的比较大的开销，多线程模型更加轻量一些。

一个进程包含多个线程，**同进程里的线程可以共享进程的部分资源的**，比如文件描述符列表、进程空间、代码、全局数据、堆、共享库等，这些共享资源在上下文切换时是不需要切换，而只需要切换线程的私有数据、寄存器等不共享的数据，因此同一个进程下的线程上下文切换的开销要比进程小得多。

当服务器与客户端 TCP 完成连接后，通过 `pthread_create()` 函数创建线程，然后将「已连接 Socket」的文件描述符传递给线程函数，接着在线程里和客户端进行通信，从而达到并发处理的目的。

并且我们可以使用**线程池**的方式来避免线程的频繁创建和销毁，所谓的线程池，就是提前创建若干个线程，这样当由新连接建立时，将这个已连接的 Socket 放入到一个队列里，然后线程池里的线程负责从队列中取出已连接 Socket 进程处理。

![image-20220104151955066](/Users/breeze/Library/Application Support/typora-user-images/image-20220104151955066.png)

需要注意的是，这个队列是全局的，每个线程都会操作，为了避免多线程竞争，线程在操作这个队列前要加锁。



**存在缺陷**

上面基于进程或者线程模型的，其实还是有问题的。新到来一个 TCP 连接，就需要分配一个进程或者线程，那么如果要达到 C10K，意味着要一台机器维护 1 万个连接，相当于要维护 1 万个进程/线程，操作系统就算死扛也是扛不住的。



### 5.5 I/O多路复用

既然为每个请求分配一个进程/线程的方式不合适，那有没有可能只使用一个进程来维护多个 Socket 呢？

答案是有的，那就是 **I/O 多路复用**技术。

<img src="/Users/breeze/Library/Application Support/typora-user-images/image-20220104152220057.png" alt="image-20220104152220057" style="zoom:67%;" />



一个进程虽然任一时刻只能处理一个请求，但是处理每个请求的事件时，耗时控制在 1 毫秒以内，这样 1 秒内就可以处理上千个请求，把时间拉长来看，多个请求复用了一个进程，这就是多路复用，这种思想很类似一个 CPU 并发多个进程，所以也叫做时分多路复用。



**select/poll/epoll 三个多路复用接口**

我们熟悉的 select/poll/epoll 内核提供给用户态的多路复用系统调用，**进程可以通过一个系统调用函数从内核中获取多个事件**。

select/poll/epoll 是如何获取网络事件的呢？在获取事件时，先把所有连接（文件描述符）传给内核，再由内核返回产生了事件的连接，然后在用户态中再处理这些连接对应的请求即可。



#### 5.5.1 select/poll

select 实现多路复用的方式是：



1. 已连接的 Socket 都放到一个**文件描述符集合**；
2. 然后调用 select 函数将文件描述符集合**拷贝**到内核里，让内核通过**遍历**文件描述符集合的方式检查是否有网络事件产生；
3. 当检查到有事件产生后，将此 Socket 标记为可读或可写；
4. 接着再把整个文件描述符集合**拷贝**回用户态里；
5. 然后用户态还需要再通过**遍历**的方法找到可读或可写的 Socket，然后再对其处理。



所以，对于 select 这种方式，需要进行 **2 次「遍历」文件描述符集合**，一次是在内核态里，一个次是在用户态里 ，而且还会发生 **2 次「拷贝」文件描述符集合**，先从用户空间传入内核空间，由内核修改后，再传出到用户空间中。



**局限**

select **使用固定长度的 BitsMap，表示文件描述符集合，而且所支持的文件描述符的个数是有限制的**，在 Linux 系统中，由内核中的 FD_SETSIZE 限制， 默认最大值为 `1024`，只能监听 0~1023 的文件描述符。

poll 不再用 BitsMap 来存储所关注的文件描述符，**取而代之用动态数组，以链表形式来组织，突破了 select 的文件描述符个数限制**，当然还会受到系统文件描述符限制。



但是 poll 和 select 并没有太大的本质区别，**都是使用「线性结构」存储进程关注的 Socket 集合，因此都需要遍历文件描述符集合来找到可读或可写的 Socket，时间复杂度为 O(n)，而且也需要在用户态与内核态之间拷贝文件描述符集合**，这种方式随着并发数上来，性能的损耗会呈指数级增长。





#### 5.5.2 epoll

epoll 通过两个方面，很好解决了 select/poll 的问题。

***第一点***

epoll 在内核里使用**红黑树来跟踪进程所有待检测的文件描述符**，把需要监控的 socket 通过 `epoll_ctl()` 函数加入内核中的红黑树里，红黑树是个高效的数据结构，增删查一般时间复杂度是 `O(logn)`，通过对这棵黑红树进行操作，这样就不需要像 select/poll 每次操作时都传入整个 socket 集合，只需要传入一个待检测的 socket，减少了内核和用户空间大量的数据拷贝和内存分配。

***第二点***

epoll 使用**事件驱动**的机制，内核里**维护了一个链表来记录就绪事件**，当某个 socket 有事件发生时，通过回调函数内核会将其加入到这个就绪事件列表中，当用户调用 `epoll_wait()` 函数时，只会返回有事件发生的文件描述符的个数，不需要像 select/poll 那样轮询扫描整个 socket 集合，大大提高了检测的效率。



从下图你可以看到 epoll 相关的接口作用：

![image-20220104172755208](/Users/breeze/Library/Application Support/typora-user-images/image-20220104172755208.png)

epoll 的方式即使监听的 Socket 数量越多的时候，效率不会大幅度降低，能够同时监听的 Socket 的数目也非常的多了，上限就为系统定义的进程打开的最大文件描述符个数。因而，**epoll 被称为解决 C10K 问题的利器**。





epoll 支持两种事件触发模式，分别是**边缘触发（\*edge-triggered，ET\*）**和**水平触发（\*level-triggered，LT\*）**。

这两个术语还挺抽象的，其实它们的区别还是很好理解的。

- 使用边缘触发模式时，当被监控的 Socket 描述符上有可读事件发生时，**服务器端只会从 epoll_wait 中苏醒一次**，即使进程没有调用 read 函数从内核读取数据，也依然只苏醒一次，因此我们程序要保证一次性将内核缓冲区的数据读取完；
- 使用水平触发模式时，当被监控的 Socket 上有可读事件发生时，**服务器端不断地从 epoll_wait 中苏醒，直到内核缓冲区数据被 read 函数读完才结束**，目的是告诉我们有数据需要读取；

举个例子，你的快递被放到了一个快递箱里，如果快递箱只会通过短信通知你一次，即使你一直没有去取，它也不会再发送第二条短信提醒你，这个方式就是边缘触发；如果快递箱发现你的快递没有被取出，它就会不停地发短信通知你，直到你取出了快递，它才消停，这个就是水平触发的方式。

这就是两者的区别，水平触发的意思是只要满足事件的条件，比如内核中有数据需要读，就一直不断地把这个事件传递给用户；而边缘触发的意思是只有第一次满足条件的时候才触发，之后就不会再传递同样的事件了。

如果使用水平触发模式，当内核通知文件描述符可读写时，接下来还可以继续去检测它的状态，看它是否依然可读或可写。所以在收到通知后，没必要一次执行尽可能多的读写操作。

如果使用边缘触发模式，I/O 事件发生时只会通知一次，而且我们不知道到底能读写多少数据，所以在收到通知后应尽可能地读写数据，以免错失读写的机会。因此，我们会**循环**从文件描述符读写数据，那么如果文件描述符是阻塞的，没有数据可读写时，进程会阻塞在读写函数那里，程序就没办法继续往下执行。所以，**边缘触发模式一般和非阻塞 I/O 搭配使用**，程序会一直执行 I/O 操作，直到系统调用（如 `read` 和 `write`）返回错误，错误类型为 `EAGAIN` 或 `EWOULDBLOCK`。

一般来说，边缘触发的效率比水平触发的效率要高，因为边缘触发可以减少 epoll_wait 的系统调用次数，系统调用也是有一定的开销的的，毕竟也存在上下文的切换。

select/poll 只有水平触发模式，epoll 默认的触发模式是水平触发，但是可以根据应用场景设置为边缘触发模式。

另外，使用 I/O 多路复用时，最好搭配非阻塞 I/O 一起使用，Linux 手册关于 select 的内容中有如下说明：

> Under Linux, select() may report a socket file descriptor as "ready for reading", while nevertheless a subsequent read blocks. This could for example happen when data has arrived but upon examination has wrong checksum and is discarded. There may be other circumstances in which a file descriptor is spuriously reported as ready. Thus it may be safer to use O_NONBLOCK on sockets that should not block.

谷歌翻译的结果：

> 在Linux下，select() 可能会将一个 socket 文件描述符报告为 "准备读取"，而后续的读取块却没有。例如，当数据已经到达，但经检查后发现有错误的校验和而被丢弃时，就会发生这种情况。也有可能在其他情况下，文件描述符被错误地报告为就绪。因此，在不应该阻塞的 socket 上使用 O_NONBLOCK 可能更安全。

简单点理解，就是**多路复用 API 返回的事件并不一定可读写的**，如果使用阻塞 I/O， 那么在调用 read/write 时则会发生程序阻塞，因此最好搭配非阻塞 I/O，以便应对极少数的特殊情况。





### 5.6 总结

最基础的 TCP 的 Socket 编程，它是阻塞 I/O 模型，基本上只能一对一通信，那为了服务更多的客户端，我们需要改进网络 I/O 模型。

比较传统的方式是使用多进程/线程模型，每来一个客户端连接，就分配一个进程/线程，然后后续的读写都在对应的进程/线程，这种方式处理 100 个客户端没问题，但是当客户端增大到 10000  个时，10000 个进程/线程的调度、上下文切换以及它们占用的内存，都会成为瓶颈。

为了解决上面这个问题，就出现了 I/O 的多路复用，可以只在一个进程里处理多个文件的  I/O，Linux 下有三种提供 I/O 多路复用的 API，分别是：select、poll、epoll。

select 和 poll 并没有本质区别，它们内部都是使用「线性结构」来存储进程关注的 Socket 集合。

在使用的时候，首先需要把关注的 Socket 集合通过 select/poll 系统调用从用户态拷贝到内核态，然后由内核检测事件，当有网络事件产生时，内核需要遍历进程关注 Socket 集合，找到对应的 Socket，并设置其状态为可读/可写，然后把整个 Socket 集合从内核态拷贝到用户态，用户态还要继续遍历整个 Socket 集合找到可读/可写的 Socket，然后对其处理。

很明显发现，select 和 poll 的缺陷在于，当客户端越多，也就是 Socket 集合越大，Socket 集合的遍历和拷贝会带来很大的开销，因此也很难应对 C10K。

epoll 是解决 C10K 问题的利器，通过两个方面解决了 select/poll 的问题。

- epoll 在内核里使用「红黑树」来关注进程所有待检测的 Socket，红黑树是个高效的数据结构，增删查一般时间复杂度是 O(logn)，通过对这棵黑红树的管理，不需要像 select/poll 在每次操作时都传入整个 Socket 集合，减少了内核和用户空间大量的数据拷贝和内存分配。
- epoll 使用事件驱动的机制，内核里维护了一个「链表」来记录就绪事件，只将有事件发生的 Socket 集合传递给应用程序，不需要像 select/poll 那样轮询扫描整个集合（包含有和无事件的 Socket ），大大提高了检测的效率。

而且，epoll 支持边缘触发和水平触发的方式，而 select/poll 只支持水平触发，一般而言，边缘触发的方式会比水平触发的效率高。
